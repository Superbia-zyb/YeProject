07/11/2023 13:16:27 - INFO - __main__ - Distributed environment: DistributedType.MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16

07/11/2023 13:16:27 - INFO - __main__ - Set random seed as 42
07/11/2023 13:16:39 - INFO - __main__ - Success for loading components and model!
07/11/2023 13:16:39 - INFO - __main__ - The mixed precision is torch.bfloat16
07/11/2023 13:16:40 - INFO - __main__ - The requires_grad of all the params is False
07/11/2023 13:16:40 - INFO - __main__ - The data type of all the params is torch.bfloat16
07/11/2023 13:16:41 - INFO - __main__ - Unfreezing the UNet params
07/11/2023 13:16:41 - INFO - __main__ - Creating EMA for the unet
07/11/2023 13:16:41 - INFO - __main__ - set torch.backends.cuda.matmul.allow_tf32 True
07/11/2023 13:16:41 - INFO - __main__ - Setup the optimizer
07/11/2023 13:16:41 - INFO - __main__ - Defining the transforms for test and train data
07/11/2023 13:16:41 - INFO - __main__ - The Dataset length is 5
07/11/2023 13:16:41 - INFO - __main__ - define scheduler for optimizer
07/11/2023 13:16:43 - INFO - __main__ - Prepare everything with diffuser accelerator
07/11/2023 13:16:43 - INFO - __main__ - ***** Running training *****
07/11/2023 13:16:43 - INFO - __main__ -   Num examples = 5
07/11/2023 13:16:43 - INFO - __main__ -   Num Epochs = 50000
07/11/2023 13:16:43 - INFO - __main__ -   Instantaneous batch size per device = 1
07/11/2023 13:16:43 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 2
07/11/2023 13:16:43 - INFO - __main__ -   Gradient Accumulation steps = 1
07/11/2023 13:16:43 - INFO - __main__ -   Total optimization steps = 150000
07/11/2023 13:16:43 - INFO - __main__ - Starting Training !!!
