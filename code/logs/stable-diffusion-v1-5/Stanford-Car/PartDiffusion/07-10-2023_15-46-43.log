07/10/2023 15:46:43 - INFO - __main__ - Distributed environment: DistributedType.MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16

07/10/2023 15:46:43 - INFO - __main__ - Set random seed as 42
07/10/2023 15:46:54 - INFO - __main__ - Success for loading components and model!
07/10/2023 15:46:54 - INFO - __main__ - The mixed precision is torch.bfloat16
07/10/2023 15:46:55 - INFO - __main__ - The requires_grad of all the params is False
07/10/2023 15:46:55 - INFO - __main__ - The data type of all the params is torch.bfloat16
07/10/2023 15:46:56 - INFO - __main__ - Unfreezing the UNet params
07/10/2023 15:46:56 - INFO - __main__ - Creating EMA for the unet
07/10/2023 15:46:56 - INFO - __main__ - set torch.backends.cuda.matmul.allow_tf32 True
07/10/2023 15:46:56 - INFO - __main__ - Setup the optimizer
07/10/2023 15:46:56 - INFO - __main__ - Defining the transforms for test and train data
07/10/2023 15:46:56 - INFO - __main__ - The Dataset length is 5
07/10/2023 15:46:57 - INFO - __main__ - define scheduler for optimizer
07/10/2023 15:46:58 - INFO - __main__ - Prepare everything with diffuser accelerator
07/10/2023 15:46:58 - INFO - __main__ - ***** Running training *****
07/10/2023 15:46:58 - INFO - __main__ -   Num examples = 5
07/10/2023 15:46:58 - INFO - __main__ -   Num Epochs = 50000
07/10/2023 15:46:58 - INFO - __main__ -   Instantaneous batch size per device = 1
07/10/2023 15:46:58 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 2
07/10/2023 15:46:58 - INFO - __main__ -   Gradient Accumulation steps = 1
07/10/2023 15:46:58 - INFO - __main__ -   Total optimization steps = 150000
07/10/2023 15:46:58 - INFO - __main__ - Starting Training !!!
